import * as webllm from "@mlc-ai/web-llm";

import { ChatOptions, LLMApi } from "./api";
import { ChatCompletionMessageParam } from "@mlc-ai/web-llm";

export class WebLLMApi implements LLMApi {
  private currentModel = "";
  private engine: webllm.EngineInterface = new webllm.Engine();

  async initModel(
    model: string,
    onUpdate?: (message: string, chunk: string) => void,
  ) {
    this.currentModel = model;
    this.engine.setInitProgressCallback((report: webllm.InitProgressReport) => {
      onUpdate?.(report.text, report.text);
    });
    await this.engine.reload(this.currentModel);
  }

  async chat(options: ChatOptions): Promise<void> {
    if (options.config.model != this.currentModel) {
      await this.initModel(options.config.model, options.onUpdate);
    }

    const reply = await this.engine.chat.completions.create({
      stream: false,
      messages: options.messages as ChatCompletionMessageParam[],
    });

    if (reply.choices[0].message.content) {
      options.onFinish(reply.choices[0].message.content);
    } else {
      options.onError?.(new Error("Empty response generated by LLM"));
    }
  }
  async usage() {
    return {
      used: 0,
      total: 0,
    };
  }
  async models() {
    return webllm.prebuiltAppConfig.model_list.map((record) => ({
      name: record.model_id,
      available: true,
      provider: {
        id: "huggingface",
        providerName: "huggingface",
        providerType: "huggingface",
      },
    }));
  }
}
