import { createContext } from "react";
import {
  InitProgressReport,
  prebuiltAppConfig,
  ChatCompletionMessageParam,
  ServiceWorkerEngine,
  ServiceWorker,
  ChatCompletionChunk,
  ChatCompletion,
} from "@neet-nestor/web-llm";

import { ChatOptions, LLMApi, LLMConfig, RequestMessage } from "./api";

const KEEP_ALIVE_INTERVAL = 10000;

export class WebLLMApi implements LLMApi {
  private llmConfig?: LLMConfig;
  engine?: ServiceWorkerEngine;

  constructor() {
    this.engine = new ServiceWorkerEngine(new ServiceWorker());
    this.engine.keepAlive(
      window.location.href + "ping.txt",
      KEEP_ALIVE_INTERVAL,
    );
  }

  async initModel(onUpdate?: (message: string, chunk: string) => void) {
    if (!this.llmConfig) {
      throw Error("llmConfig is undefined");
    }
    if (!this.engine) {
      this.engine = new ServiceWorkerEngine(new ServiceWorker());
    }
    let hasResponse = false;
    this.engine.setInitProgressCallback((report: InitProgressReport) => {
      onUpdate?.(report.text, report.text);
      hasResponse = true;
    });
    let initRequest = this.engine.init(this.llmConfig.model, this.llmConfig, {
      ...prebuiltAppConfig,
      useIndexedDBCache: this.llmConfig.cache === "index_db",
    });
    // In case the service worker is dead, init will halt indefinitely
    // so we manually retry if timeout
    let retry = 0;
    let engine = this.engine;
    let llmConfig = this.llmConfig;
    let retryInterval: NodeJS.Timeout;

    await new Promise<void>((resolve, reject) => {
      retryInterval = setInterval(() => {
        if (hasResponse) {
          clearInterval(retryInterval);
          initRequest.then(resolve);
          return;
        }
        if (retry >= 5) {
          clearInterval(retryInterval);
          reject("Model initialization timed out for too many times");
          return;
        }
        retry += 1;
        initRequest = engine.init(llmConfig.model, llmConfig, {
          ...prebuiltAppConfig,
          useIndexedDBCache: llmConfig.cache === "index_db",
        });
      }, 5000);
    });
  }

  isConfigChanged(config: LLMConfig) {
    return (
      this.llmConfig?.model !== config.model ||
      this.llmConfig?.cache !== config.cache ||
      this.llmConfig?.temperature !== config.temperature ||
      this.llmConfig?.top_p !== config.top_p ||
      this.llmConfig?.presence_penalty !== config.presence_penalty ||
      this.llmConfig?.frequency_penalty !== config.frequency_penalty
    );
  }

  async chatCompletion(
    stream: boolean,
    messages: RequestMessage[],
    onUpdate?: (message: string, chunk: string) => void,
  ) {
    let reply: string | null = "";

    const completion = await this.engine!.chatCompletion({
      stream: stream,
      messages: messages as ChatCompletionMessageParam[],
    });

    if (stream) {
      const asyncGenerator = completion as AsyncIterable<ChatCompletionChunk>;
      for await (const chunk of asyncGenerator) {
        if (chunk.choices[0].delta.content) {
          reply += chunk.choices[0].delta.content;
          onUpdate?.(reply, chunk.choices[0].delta.content);
        }
      }
      return reply;
    }
    return (completion as ChatCompletion).choices[0].message.content;
  }

  async chat(options: ChatOptions): Promise<void> {
    // in case the service worker is dead, revive it by firing a fetch event
    fetch("/ping.txt");

    if (this.isConfigChanged(options.config)) {
      this.llmConfig = options.config;
      try {
        await this.initModel(options.onUpdate);
      } catch (e) {
        console.error("Error in initModel", e);
      }
    }

    let reply: string | null = "";
    try {
      reply = await this.chatCompletion(
        !!options.config.stream,
        options.messages,
        options.onUpdate,
      );
    } catch (err: any) {
      if (err.toString().includes("Please call `Engine.reload(model)` first")) {
        console.error("Error in chatCompletion", err);
        options.onError?.(err as Error);
        return;
      }
      // Service worker has been stopped. Restart it
      await this.initModel(options.onUpdate);
      reply = await this.chatCompletion(
        !!options.config.stream,
        options.messages,
        options.onUpdate,
      );
    }

    if (reply) {
      options.onFinish(reply);
    } else {
      options.onError?.(new Error("Empty response generated by LLM"));
    }
  }

  async abort() {
    await this.engine?.interruptGenerate();
  }

  async usage() {
    return {
      used: 0,
      total: 0,
    };
  }
}

export const WebLLMContext = createContext<WebLLMApi | null>(null);
