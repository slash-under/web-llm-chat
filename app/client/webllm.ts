import { createContext } from "react";
import {
  InitProgressReport,
  prebuiltAppConfig,
  ChatCompletionMessageParam,
  ServiceWorkerEngine,
  ChatCompletionChunk,
  ChatCompletion,
} from "@neet-nestor/web-llm";

import { ChatOptions, LLMApi, LLMConfig, RequestMessage } from "./api";

const KEEP_ALIVE_INTERVAL = 5_000;

export class WebLLMApi implements LLMApi {
  private llmConfig?: LLMConfig;
  engine: ServiceWorkerEngine;

  constructor() {
    if (!("serviceWorker" in navigator)) {
      throw Error("Service worker API is not available");
    }
    if (!navigator.serviceWorker.controller) {
      throw Error("There is no active service worker");
    }
    this.engine = new ServiceWorkerEngine(
      navigator.serviceWorker.controller,
      KEEP_ALIVE_INTERVAL,
    );
  }

  async initModel(onUpdate?: (message: string, chunk: string) => void) {
    if (!this.llmConfig) {
      throw Error("llmConfig is undefined");
    }
    this.engine.setInitProgressCallback((report: InitProgressReport) => {
      onUpdate?.(report.text, report.text);
    });
    await this.engine.init(this.llmConfig.model, this.llmConfig, {
      ...prebuiltAppConfig,
      useIndexedDBCache: this.llmConfig.cache === "index_db",
    });
  }

  isConfigChanged(config: LLMConfig) {
    return (
      this.llmConfig?.model !== config.model ||
      this.llmConfig?.cache !== config.cache ||
      this.llmConfig?.temperature !== config.temperature ||
      this.llmConfig?.top_p !== config.top_p ||
      this.llmConfig?.presence_penalty !== config.presence_penalty ||
      this.llmConfig?.frequency_penalty !== config.frequency_penalty
    );
  }

  async chat(options: ChatOptions): Promise<void> {
    if (this.isDifferentConfig(options.config)) {
      this.llmConfig = { ...(this.llmConfig || {}), ...options.config };
      try {
        await this.initModel(options.onUpdate);
      } catch (e) {
        console.error("Error while initializing the model", e);
      }
    }

    let reply: string | null = "";
    try {
      reply = await this.chatCompletion(
        !!options.config.stream,
        options.messages,
        options.onUpdate,
      );
    } catch (err: any) {
      if (
        !err.toString().includes("Please call `Engine.reload(model)` first")
      ) {
        console.error("Error in chatCompletion", err);
        options.onError?.(err as Error);
        return;
      }
      // Service worker has been stopped. Restart it
      await this.initModel(options.onUpdate);
      reply = await this.chatCompletion(
        !!options.config.stream,
        options.messages,
        options.onUpdate,
      );
    }

    if (reply) {
      options.onFinish(reply);
    } else {
      options.onError?.(new Error("Empty response generated by LLM"));
    }
  }

  async abort() {
    await this.engine?.interruptGenerate();
  }

  async usage() {
    return {
      used: 0,
      total: 0,
    };
  }

  isDifferentConfig(config: LLMConfig): boolean {
    if (!this.llmConfig) {
      return true;
    }

    // Compare required fields
    if (this.llmConfig.model !== config.model) {
      return true;
    }

    // Compare optional fields
    const optionalFields: (keyof LLMConfig)[] = [
      "temperature",
      "top_p",
      "stream",
      "presence_penalty",
      "frequency_penalty",
    ];

    for (const field of optionalFields) {
      if (
        this.llmConfig[field] !== undefined &&
        config[field] !== undefined &&
        config[field] !== config[field]
      ) {
        return true;
      }
    }

    return false;
  }

  async chatCompletion(
    stream: boolean,
    messages: RequestMessage[],
    onUpdate?: (message: string, chunk: string) => void,
  ) {
    let reply: string | null = "";

    const completion = await this.engine!.chatCompletion({
      stream: stream,
      messages: messages as ChatCompletionMessageParam[],
    });

    if (stream) {
      const asyncGenerator = completion as AsyncIterable<ChatCompletionChunk>;
      for await (const chunk of asyncGenerator) {
        if (chunk.choices[0].delta.content) {
          reply += chunk.choices[0].delta.content;
          onUpdate?.(reply, chunk.choices[0].delta.content);
        }
      }
      return reply;
    }
    return (completion as ChatCompletion).choices[0].message.content;
  }
}

export const WebLLMContext = createContext<WebLLMApi | null>(null);
