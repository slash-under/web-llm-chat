import * as webllm from "@mlc-ai/web-llm";

import { ChatOptions, LLMApi } from "./api";
import { ChatCompletionMessageParam } from "@mlc-ai/web-llm";

export class WebLLMApi implements LLMApi {
  private currentModel?: string;
  private engine?: webllm.EngineInterface;

  async initModel(
    model: string,
    onUpdate?: (message: string, chunk: string) => void,
  ) {
    this.currentModel = model;
    this.engine = await webllm.CreateWebWorkerEngine(
      new Worker(new URL("./webllm-sw.ts", import.meta.url), {
        type: "module",
      }),
      this.currentModel,
      {
        initProgressCallback: (report: webllm.InitProgressReport) => {
          onUpdate?.(report.text, report.text);
        },
      },
    );
  }

  async chat(options: ChatOptions): Promise<void> {
    console.log(this.currentModel, options.config.model);

    if (options.config.model !== this.currentModel) {
      await this.initModel(options.config.model, options.onUpdate);
    }

    const reply = await this.engine!.chat.completions.create({
      stream: false,
      messages: options.messages as ChatCompletionMessageParam[],
    });

    if (reply.choices[0].message.content) {
      options.onFinish(reply.choices[0].message.content);
    } else {
      options.onError?.(new Error("Empty response generated by LLM"));
    }
  }
  async usage() {
    return {
      used: 0,
      total: 0,
    };
  }
  async models() {
    return webllm.prebuiltAppConfig.model_list.map((record) => ({
      name: record.model_id,
      available: true,
      provider: {
        id: "huggingface",
        providerName: "huggingface",
        providerType: "huggingface",
      },
    }));
  }
}
